# -*- coding: utf-8 -*-
"""Building a neural network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MAVyuuzGD2r4dTST0gOdTEbBy_W0cwoa
"""

X=[[2,23,25,90],[12,33,5,91],[62,73,15,9]]


weights=[[2.71,2.23,3.09,8.7],[2,67,89,99],[12,34,11,10]]
biases=[9,4,2]

weights2=[[8.71,12.23,33.09,28.7],[52,67,9,99],[2,34,1,10]]
biases2=[7,3,5]



import numpy as np
np.random.seed(0)
X=[[2,23,25,90],[12,33,5,91],[62,73,15,9]]

class Layer_Dense:
    def __init__(self, n_inputs, n_neurons):
         self.weights=0.10*np.random.randn(n_inputs, n_neurons)
         self.biases=np.zeros((1,n_neurons))
    def forward(self, inputs):
       self.output=np.dot(inputs, self.weights) + self.biases


layer1=Layer_Dense(4,5)
layer2=Layer_Dense(5,2)

layer1.forward(X)
print(layer1.output)
layer2.forward(layer1.output)
print(layer2.output)
#print(layeroutput2)

import numpy as np

# Generate an array of random numbers from a standard normal distribution (mean=0, std=1)
random_numbers = np.random.randn(5)

import tensorflow as tf

# Check if GPU is available
print("GPU is", "available" if tf.config.list_physical_devices('GPU') else "NOT AVAILABLE")

"""**How Step Functions work**"""

#Defining our function
def step_function(x):
    return 1 if x >= 0 else 0

# Testing the step function out
print(step_function(14))
print(step_function(-8))

"""**How RELU activation functions work**



"""

#Defining our function
def relu(x):
    return max(0, x)

# Testing ReLU out
print(relu(14))
print(relu(-8))

"""**How Sigmoid functions work**"""

#importing the library we need for this
import math
#defining our function with the mathematical formula for this
def sigmoid(x):
    return 1 / (1 + math.exp(-x))

# Testing  the sigmoid out:)
print(sigmoid(14))
print(sigmoid(-8))

